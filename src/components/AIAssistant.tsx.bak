import React, { useState, useEffect, useRef } from 'react';
import { explainCode, readCodeAloud, answerCodeQuestion, GeminiResponse } from '../services/geminiService';
// Using Google Cloud TTS for better voice quality with credentials now available
import googleTTSService from '../services/googleTTSService';
// Keep fakeTTSService as fallback
import fakeTTSService from '../services/fakeTTSService';

interface Message {
  id: string;
  sender: 'user' | 'ai';
  text: string;
  timestamp: Date;
  annotations?: GeminiResponse['annotations'];
  isProcessing?: boolean;
}

interface AIAssistantProps {
  code: string;
  language: string;
  onHighlight: (lineNumber: number) => void;
  highlightedCode?: string;
  highlightedLines?: { start: number; end: number };
}

// Define the WebkitSpeechRecognition interface for voice input
interface IWebkitSpeechRecognition extends EventTarget {
  continuous: boolean;
  interimResults: boolean;
  lang: string;
  start: () => void;
  stop: () => void;
  onresult: (event: any) => void;
  onend: () => void;
  onerror: (event: any) => void;
}

declare global {
  interface Window {
    webkitSpeechRecognition: any;
  }
}

const AIAssistant: React.FC<AIAssistantProps> = ({
  code,
  language,
  onHighlight,
  highlightedCode,
  highlightedLines
}) => {
  // Debug received props
  useEffect(() => {
    console.log('AIAssistant received props:', { 
      codeLength: code?.length,
      language,
      hasHighlightedCode: !!highlightedCode
    });
  }, [code, language, highlightedCode]);
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState('');
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [currentHighlight, setCurrentHighlight] = useState<number>(-1);
  const messagesEndRef = useRef<HTMLDivElement>(null);
  const [isRecording, setIsRecording] = useState(false);
  const [recognition, setRecognition] = useState<IWebkitSpeechRecognition | null>(null);
  
  // Track active speech segment for highlighting
  const [activeSpeechSegment, setActiveSpeechSegment] = useState<number>(-1);

  // Format timestamp
  const formatTime = (date: Date) => {
    return date.toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' });
  };

  // Development mode flag to show debugging panel
  const [showDebug, setShowDebug] = useState(true);

  // Initialize speech recognition if available
  useEffect(() => {
    if (typeof window !== 'undefined' && 'webkitSpeechRecognition' in window) {
      // Cast the global object to our interface
      const SpeechRecognition = window.webkitSpeechRecognition as unknown as {
        new (): IWebkitSpeechRecognition;
      };
      
      const recognitionInstance = new SpeechRecognition();
      recognitionInstance.continuous = false;
      recognitionInstance.interimResults = false;
      recognitionInstance.lang = 'en-US';
      
      recognitionInstance.onresult = (event: any) => {
        const transcript = event.results[0][0].transcript;
        setInput(transcript);
        // Auto-submit voice input
        handleSendMessage(transcript);
      };
      
      recognitionInstance.onend = () => {
        setIsRecording(false);
      };
      
      recognitionInstance.onerror = (event: any) => {
        console.error('Speech recognition error', event.error);
        setIsRecording(false);
      };
      
      setRecognition(recognitionInstance);
    }
  }, []);

  // Scroll to bottom when messages change
  useEffect(() => {
    // Add a small delay to ensure the DOM has updated
    setTimeout(() => {
      scrollToBottom();
    }, 50);
  }, [messages]);

  // Add welcome message on first load
  useEffect(() => {
    if (messages.length === 0) {
      setMessages([
        {
          id: 'welcome',
          sender: 'ai',
          text: 'Hi! I\'m Glyssa, your AI code tutor. I can explain your code, answer questions, and read code aloud. How can I help you today?',
          timestamp: new Date(),
        }
      ]);
    }
  }, [messages.length]);

  // Initialize TTS service on component mount
  useEffect(() => {
    // Initialize the Google Cloud TTS service
    googleTTSService.initVoices().then((success: boolean) => {
      if (!success) {
        console.error('Failed to initialize Google TTS voices, falling back to browser TTS');
        // Fall back to browser-based TTS
        fakeTTSService.initVoices().catch((error) => {
          console.error('Failed to initialize fallback TTS voices', error);
        });
      }
    }).catch((error) => {
      console.error('Error initializing Google TTS voices', error);
      // Fall back to browser-based TTS
      fakeTTSService.initVoices().catch((error) => {
        console.error('Failed to initialize fallback TTS voices', error);
      });
    });
    
    // Cleanup on unmount
    return () => {
      googleTTSService.stop();
      fakeTTSService.stop();
    };
  }, []);

  const scrollToBottom = () => {
    // Prevent the chat from forcing the editor to scroll up
    if (messagesEndRef.current) {
      // Use a more contained approach to scrolling that doesn't affect the parent container
      const chatContainer = messagesEndRef.current.parentElement;
      if (chatContainer) {
        chatContainer.scrollTop = chatContainer.scrollHeight;
      }
    }
  };

  // Handle sending a message
  const handleSendMessage = async (messageText = input) => {
    if (!messageText.trim()) return;
    
    // Add user message
    const userMessage: Message = {
      id: Date.now().toString(),
      sender: 'user',
      text: messageText,
      timestamp: new Date(),
    };
    
    // Add AI processing message
    const processingId = Date.now().toString() + '-processing';
    const processingMessage: Message = {
      id: processingId,
      sender: 'ai',
      text: '...',
      timestamp: new Date(),
      isProcessing: true,
    };
    
    setMessages(prev => [...prev, userMessage, processingMessage]);
    setInput('');
    
    try {
      let response: GeminiResponse;
      
      // Determine the type of request
      const lowerText = messageText.toLowerCase();
      
      if (lowerText.includes('read') && lowerText.includes('aloud')) {
        // Read code aloud request - ensure we're using the actual current code
        console.log('Sending code to read aloud:', { codeLength: code?.length, language });
        response = await readCodeAloud({ code, language });
        
        // Read the response aloud
        if (response && response.text) {
          handleReadAloud(
            processingId,
            response.text,
            response.annotations,
            code // Pass the actual code for line validation
          );
        }
        
      } else if (lowerText.includes('explain') || 
                lowerText.includes('what') || 
                lowerText.includes('how') || 
                lowerText.includes('why')) {
        
        if (highlightedCode) {
          // If code is highlighted, answer question about highlighted code
          response = await answerCodeQuestion(highlightedCode, messageText, language, highlightedCode);
        } else {
          // General code explanation
          response = await explainCode({ code, language });
        }
        
        // Read the response aloud
        if (response && response.text) {
          handleReadAloud(
            processingId,
            response.text,
            response.annotations,
            code // Pass the actual code for line validation
          );
        }
        
      } else {
        // Default to answering a question about the code
        response = await answerCodeQuestion(code, messageText, language, undefined);
        
        // Read the response aloud
        if (response && response.text) {
          handleReadAloud(
            processingId,
            response.text,
            response.annotations,
            code // Pass the actual code for line validation
          );
        }
      }
      
      // Update the processing message with the real response
      setMessages(prev => 
        prev.map(msg => 
          msg.id === processingId 
            ? { 
                ...msg, 
                text: response.text, 
                annotations: response.annotations,
                isProcessing: false 
              } 
            : msg
        )
      );
      
    } catch (error) {
      console.error('Error getting AI response:', error);
      
      // Update the processing message with the error
      setMessages(prev => 
        prev.map(msg => 
          msg.id === processingId 
            ? { 
                ...msg, 
                text: 'Sorry, I encountered an error processing your request. Please try again.', 
                isProcessing: false 
              } 
            : msg
        )
      );
    }
  };
  
  // Function to prepare code annotations and organize them for speech
  const prepareAnnotationsForSpeech = (annotations?: GeminiResponse['annotations']) => {
    if (!annotations || annotations.length === 0) return [];
    
    // Sort annotations by start line
    return [...annotations].sort((a, b) => a.startLine - b.startLine);
  };
  
  // Handle reading code aloud with enhanced experience
  const handleReadAloud = (messageId: string, text: string, annotations?: GeminiResponse['annotations'], codeContent?: string) => {
    // Stop any previous speech
    googleTTSService.stop();
    fakeTTSService.stop();
    
    // Set state for UI updates
    setIsSpeaking(true);
    setActiveSpeechSegment(-1);
    
    console.log('Handle read aloud called with:', { messageId, textLength: text?.length, hasAnnotations: !!annotations, codeContent: codeContent?.substring(0, 100) });
    
    // Ensure we have actual code content to work with
    const actualCode = codeContent || code;
    if (!actualCode) {
      console.error('No code content available for read aloud');
      setIsSpeaking(false);
      return;
    }
    
    // Find all [Line X] references in the text - including formats with and without spaces
    const lineRefs: { index: number; lineNumber: number; text: string }[] = [];
    // Updated regex to match both [Line X] and [LineX] formats
    const regex = /\[Line\s*(\d+)\]/g;
    let match;
    
    // Debug the text to check for line references
    console.log('Analyzing text for line references:', text.substring(0, 100) + '...');
    
    // Format the text for better speech - add pauses and emphasis
    let processedText = text
      // Add slight pauses after sentences for more natural speech
      .replace(/\.\.\./g, ' <break time="1s"/> ')
      .replace(/\./g, '. <break time="0.5s"/> ')
      // Emphasize line numbers for clarity
      .replace(/\[Line\s*(\d+)\]/g, '<emphasis level="strong">Line $1</emphasis>');
    
    console.log('Processed text for TTS:', processedText.substring(0, 200));
    
    // Extract line number references and validate against actual code
    const originalText = text; // Keep a copy of the original text for regex matching
    const codeLines = actualCode.split('\n');
    console.log(`Code has ${codeLines.length} lines`); 

    // Test the regex against the text to debug
    const lineMatches = text.match(regex);
    console.log('Line references found:', lineMatches ? lineMatches.length : 0);
    
    // If no line references are found in the text, generate them for each non-empty line
    if (!lineMatches) {
      console.log('No explicit line references found in text, generating references for code lines');
      // Create synthetic line references for each code line
      for (let i = 0; i < codeLines.length; i++) {
        const line = codeLines[i].trim();
        if (line && !line.startsWith('//') && !line.startsWith('/*') && !line.startsWith('*')) {
          lineRefs.push({
            index: i,
            lineNumber: i, // 0-indexed line numbers
            text: `[Line ${i + 1}]: ${line.substring(0, 30)}${line.length > 30 ? '...' : ''}`
          });
          console.log(`Added synthetic line reference for line ${i + 1}: ${line.substring(0, 30)}${line.length > 30 ? '...' : ''}`);
        }
      }
    } else {
      // Process explicit line references from the text
      console.log('Processing explicit line references in text');
      regex.lastIndex = 0; // Reset the regex to ensure we start from the beginning
      while ((match = regex.exec(originalText)) !== null) {
        const lineNumber = parseInt(match[1], 10);
        // Make sure we have valid line numbers by checking against the actual code
        if (!isNaN(lineNumber) && lineNumber > 0) {
          // Ensure the line number is within the range of the code
          if (lineNumber <= codeLines.length) {
            // Get surrounding context for a better highlight description
            const contextStart = Math.max(0, match.index - 20);
            const contextEnd = Math.min(originalText.length, match.index + match[0].length + 50);
            const context = originalText.substring(match.index + match[0].length, contextEnd);
            
            lineRefs.push({
              index: match.index,
              lineNumber: lineNumber - 1, // Adjust for 0-indexed line numbers
              text: match[0] + (context ? ": " + context.split('.')[0] : "")
            });
            console.log(`Found line reference at line ${lineNumber}: ${codeLines[lineNumber-1].substring(0, 30)}`);
          } else {
            console.warn(`Line reference ${lineNumber} is out of range (max: ${codeLines.length})`);
          }
        }
      }
    }
    
    // If we don't have any valid line refs but we do have annotations, use them directly
    if (lineRefs.length === 0 && annotations && annotations.length > 0) {
      console.log('Using annotations directly for highlighting:', annotations);
      annotations.forEach((annotation, index) => {
        // The annotations are already 0-indexed from our fallback service
        lineRefs.push({
          index,
          lineNumber: annotation.startLine, // Already 0-indexed
        
  // If we don't have any valid line refs but we do have annotations, use them directly
  if (lineRefs.length === 0 && annotations && annotations.length > 0) {
    console.log('Using annotations directly for highlighting:', annotations);
    annotations.forEach((annotation, index) => {
      // The annotations are already 0-indexed from our fallback service
      lineRefs.push({
        index,
        lineNumber: annotation.startLine, // Already 0-indexed
        text: `Line ${annotation.startLine + 1}: ${annotation.explanation}`
      });
    });
    console.log('Created line refs from annotations:', lineRefs);
  }
            // If we get a line number callback, use it
            if (lineNumber >= 0) {
              console.log(`Highlighting line ${lineNumber} from TTS callback`);
              onHighlight(lineNumber);
              setCurrentHighlight(lineNumber);
              
              // Find the corresponding segment
              const segmentIndex = lineRefs.findIndex(ref => ref.lineNumber === lineNumber);
              if (segmentIndex >= 0) {
                setActiveSpeechSegment(segmentIndex);
              }
            }
          },
          () => {
            // On finish
            clearInterval(annotationInterval);
            setIsSpeaking(false);
            setActiveSpeechSegment(-1);
            setCurrentHighlight(-1);
          },
          undefined, // Use default voice options
          codeToRead // Pass the actual code content for line validation
        ).catch((error) => {
          console.error('Google TTS error, falling back:', error);
          setIsSpeaking(false);
          setActiveSpeechSegment(-1);
          setCurrentHighlight(-1);
        });
      } catch (error) {
        console.error('Error with Google TTS, falling back:', error);
        
        // If Google TTS fails, fall back to browser-based TTS with line highlighting
        fakeTTSService.speak(
          processedText,
          (lineNumber: number) => {
            onHighlight(lineNumber);
            setCurrentHighlight(lineNumber);
          },
          () => {
            // Clean up interval when speech ends
            setIsSpeaking(false);
            setActiveSpeechSegment(-1);
            setCurrentHighlight(-1);
          }
        );
      }
        
    } else {
      // No line references, try to use annotations for highlighting if available
      if (annotations && annotations.length > 0) {
        const sortedAnnotations = prepareAnnotationsForSpeech(annotations);
        
        // Try Google Cloud TTS first
        googleTTSService.speak(
          processedText,
          () => {}, // Empty callback for lineNumber
          () => {
            setIsSpeaking(false);
          }
        ).catch(() => {
          // Fallback to browser TTS
          fakeTTSService.speak(
            processedText,
            () => {}, // Empty callback for lineNumber
            () => {
              setIsSpeaking(false);
              setActiveSpeechSegment(-1);
              setCurrentHighlight(-1);
            }
          );
        });
        
      } else {
        // No annotations either, just read the text with Google Cloud TTS
        googleTTSService.speak(
          processedText,
          () => {}, // Empty callback for lineNumber
          () => {
            setIsSpeaking(false);
          }
        ).catch(() => {
          // Fallback to browser TTS
          fakeTTSService.speak(
            processedText,
            () => {}, // Empty callback for lineNumber
            () => {
              setIsSpeaking(false);
            }
          );
        });
      }
    }
  };

  // Toggle voice input recording
  const toggleRecording = () => {
    if (!recognition) {
      console.error('Speech recognition not available');
      return;
    }
    
    if (isRecording) {
      recognition.stop();
      setIsRecording(false);
    } else {
      recognition.start();
      setIsRecording(true);
    }
  };

  return (
    <div className="ai-assistant-container" style={{ overflow: 'hidden' }}>
      {/* Debugging Panel */}
      {showDebug && (
        <div style={{ 
          padding: '10px', 
          backgroundColor: '#2a2a2a', 
          borderRadius: '4px',
          marginBottom: '10px',
          fontSize: '12px'
        }}>
          <div style={{ display: 'flex', justifyContent: 'space-between', marginBottom: '5px' }}>
            <strong>Debug Panel</strong>
            <button 
              onClick={() => setShowDebug(false)}
              style={{ background: 'none', border: 'none', color: '#aaa', cursor: 'pointer' }}
            >
              Hide
            </button>
          </div>
          <div><strong>Speaking:</strong> {isSpeaking ? 'Yes' : 'No'}</div>
          <div><strong>Current Highlight:</strong> Line {currentHighlight >= 0 ? currentHighlight + 1 : 'None'}</div>
          <div><strong>Active Segment:</strong> {activeSpeechSegment >= 0 ? activeSpeechSegment : 'None'}</div>
          
          {/* TTS Status */}
          <div className="mt-4 border-t border-zinc-700 pt-4">
            <h3 className="text-sm font-bold mb-2">TTS Status</h3>
            <div className="text-xs text-green-500">âœ… Google Cloud TTS enabled</div>
          </div>
          <div>
            <button 
              onClick={() => {
                // Get the actual first lines of the current code for the test
                const codeLines = code.split('\n');
                const firstLine = codeLines.length > 0 ? codeLines[0].trim() : 'No code available';
                const secondLine = codeLines.length > 1 ? codeLines[1].trim() : 'No second line';
                
                // Test TTS with actual code context
                handleReadAloud(
                  'test', 
                  `This is a test of the TTS system using Google Cloud. [Line 1] ${firstLine}. [Line 2] ${secondLine}.`,
                  [{
                    startLine: 1,
                    endLine: 1,
                    explanation: firstLine
                  }, {
                    startLine: 2,
                    endLine: 2,
                    explanation: secondLine
                  }],
                  code // Pass the actual code for line validation
                );
              }}
              style={{
                background: '#4a4a4a',
                border: 'none',
                color: 'white',
                padding: '5px 10px',
                borderRadius: '4px',
                marginTop: '5px',
                cursor: 'pointer'
              }}
            >
              Test TTS
            </button>
          </div>
        </div>
      )}
      
      <div className="bg-zinc-900 text-zinc-300 flex flex-col h-full" style={{ overflow: 'hidden' }}>
        {/* Messages area with thin scrollbars */}
        <div className="flex-1 p-4 overflow-y-auto space-y-4" style={{ 
          scrollbarWidth: 'thin', 
          msOverflowStyle: 'none'
        }}>
          {messages.map((message) => (
            <div key={message.id} className="mb-6">
              <div className="flex items-start">
                <div className="w-8 h-8 rounded-md bg-zinc-800 flex items-center justify-center mr-3 flex-shrink-0">
                  {message.sender === 'ai' ? (
                    <svg xmlns="http://www.w3.org/2000/svg" className="h-4 w-4 text-zinc-400" fill="none" viewBox="0 0 24 24" stroke="#fff">
                      <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={1.5} d="M9.75 17L9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />
                    </svg>
                  ) : (
                    <svg xmlns="http://www.w3.org/2000/svg" className="h-4 w-4 text-zinc-400" fill="none" viewBox="0 0 24 24" stroke="#fff">
                      <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={1.5} d="M16 7a4 4 0 11-8 0 4 4 0 018 0zM12 14a7 7 0 00-7 7h14a7 7 0 00-7-7z" />
                    </svg>
                  )}
                </div>
                
                <div className="flex-1">
                  <div className="flex justify-between">
                    <div className="font-medium text-sm text-zinc-300">{message.sender === 'ai' ? 'Glyssa' : 'You'}</div>
                    <div className="text-xs text-zinc-500">{formatTime(message.timestamp)}</div>
                  </div>
                  
                  <div className={`mt-1 text-sm ${message.sender === 'ai' ? 'text-zinc-300' : 'text-zinc-200'}`}>
                    {message.isProcessing ? (
                      <div className="flex items-center">
                        <div className="animate-pulse">Thinking...</div>
                      </div>
                    ) : (
                      message.text
                    )}
                  </div>
                  
                  {/* Show annotations if available */}
                  {message.sender === 'ai' && message.annotations && message.annotations.length > 0 && !message.isProcessing && (
                    <div className="mt-3 pl-2 border-l-2 border-zinc-700">
                      <div className="text-xs font-semibold text-zinc-500 mb-1">Code Annotations:</div>
                      <div className="space-y-1">
                        {message.annotations.map((annotation, index) => (
                          <div 
                            key={index}
                            className="p-1.5 bg-zinc-800 rounded cursor-pointer hover:bg-zinc-700"
                            onClick={() => {
                              // The annotations are already 0-indexed, so use directly
                              const lineNumber = annotation.startLine;
                              console.log('Highlighting annotation line:', lineNumber);
                              onHighlight(lineNumber);
                              setCurrentHighlight(lineNumber);
                            }}
                          >
                            Line {annotation.startLine}{annotation.endLine > annotation.startLine ? `-${annotation.endLine}` : ''}: {annotation.explanation || 'Highlighted line'}
                          </div>
                        ))}
                      </div>
                    </div>
                  )}
                </div>
              </div>
            </div>
          ))}
          <div ref={messagesEndRef} />
        </div>
        
        {/* Suggestion chips */}
        <div className="p-4 border-t border-zinc-800">
          <div className="mb-4">
            <div className="text-xs font-semibold text-zinc-500 mb-2">Try asking about:</div>
            <div className="flex flex-wrap gap-2">
              <button 
                className="px-3 py-1.5 text-xs bg-zinc-800 text-zinc-400 rounded-md hover:bg-zinc-700 transition-colors"
                onClick={() => handleSendMessage("Explain this code")}
              >
                Explain this code
              </button>
              <button 
                className="px-3 py-1.5 text-xs bg-zinc-800 text-zinc-400 rounded-md hover:bg-zinc-700 transition-colors"
                onClick={() => handleSendMessage("Read this code aloud")}
              >
                Read aloud
              </button>
              <button 
                className="px-3 py-1.5 text-xs bg-zinc-800 text-zinc-400 rounded-md hover:bg-zinc-700 transition-colors"
                onClick={() => handleSendMessage("What does this function do?")}
              >
                Function purpose
              </button>
            </div>
          </div>
          
          {/* Input area */}
          <div className="flex items-center">
            <input 
              type="text" 
              placeholder="Ask about your code..." 
              className="flex-1 px-4 py-2 text-sm text-zinc-200 bg-zinc-800 border border-zinc-700 rounded-lg focus:outline-none focus:ring-2 focus:ring-zinc-700 focus:border-transparent"
              value={input}
              onChange={(e) => setInput(e.target.value)}
              onKeyPress={(e) => e.key === 'Enter' && handleSendMessage()}
            />
            
            {/* Voice input button */}
            <button 
              className={`ml-2 p-2 rounded-lg ${isRecording ? 'bg-red-600' : 'bg-zinc-700'} text-white hover:bg-zinc-600 transition-colors`}
              onClick={toggleRecording}
              title={isRecording ? 'Stop recording' : 'Voice input'}
            >
              <svg xmlns="http://www.w3.org/2000/svg" className="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={1.5} d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
              </svg>
            </button>
            
            {/* Send button */}
            <button 
              className="ml-2 p-2 bg-zinc-700 text-white rounded-lg hover:bg-zinc-600 transition-colors"
              onClick={() => handleSendMessage()}
            >
              <svg xmlns="http://www.w3.org/2000/svg" className="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={1.5} d="M13 5l7 7-7 7M5 5l7 7-7 7" />
              </svg>
            </button>
          </div>
        </div>
      </div>
    </div>
  );
};

export default AIAssistant;
